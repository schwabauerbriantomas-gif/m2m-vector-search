# M2M Vector Search Engine

**High-performance Machine-to-Memory (M2M) Engine & Gaussian Splat Vector Cloud**

 [![Python](https://img.shields.io/badge/python-3.8%2B-brightgreen.svg)](https://python.org)
 [![License](https://img.shields.io/badge/license-Apache%202.0-yellow.svg)](LICENSE)

---

## ðŸ“– Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Benchmarks](#benchmarks)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Contributing](#contributing)
- [License](#license)

---

## ðŸŽ¯ Overview

**M2M Vector Search** is a next-generation vector database built on **Gaussian Splats** and **Tier-Aware Memory** (VRAM, RAM, SSD) capable of scaling to millions of high-dimensional vectors. 

Beyond simple static retrieval, M2M possesses an active architecture that supports **Generative Langevin Augmentations**, **Hardware-Accelerated SPIR-V MoE Routing**, and **Progressive Semantic LODs** for sub-millisecond AI context retrieval.

### ðŸ’¡ Key Capabilities
- **Massive Hierarchical Retrieval:** Route searches through coarse/fine KMeans++ macros, bypassing linear scans at scales up to 10k QPS.
- **True Vulkan Hardware Acceleration:** 100% FAISS/CUDA-free execution. Runs parallelized spatial shaders strictly on `SPIR-V` through CFFI.
- **Edge Computing Native:** Dependency-free Python bindings allowing the M2M Router to execute efficiently inside Edge IoT endpoints and smartphones.
- **Active Data Lake Training:** Feed embedded spaces directly into PyTorch scripts, bypassing RAM bottlenecks while augmenting vectors continuously via `Langevin Dynamics`.

---

## ðŸŒŸ Features

### 1. Vector Search & RAG Optimization
- **Progressive Semantic LODs:** Adaptive Semantic Routing. Exit early with LOD 0 (Coarse Centroids) for `< 1ms` Time-To-First-Frame context, or drill down to LOD 2 for precise Vulkan MoE evaluation (`~20ms`).
- **Semantic Spatial Router:** Native `KMeans++` routing combined with raw GPU Compute shaders to calculate cross-cluster Euclidean potentials in zero-copy boundaries.
- **Edge-Ready Standalone Nodes:** Capable of running dependency-free (No PyTorch/TorchVision) simply via `numpy` and `vulkan` Python FFI.

### 2. Tiered Storage Data Lake
- **Tier-Aware Streaming:** Background prefetching threads seamlessly load data from Cold (SSD) $\rightarrow$ Warm (RAM) $\rightarrow$ Hot (VRAM).
- **SOC Importance Sampling:** Train faster by letting the Self-Organized Criticality (SOC) controller feed only the most "concentrated" ($\kappa$) embeddings.
- **Gaussian Representations:** Stores mean ($\mu$), concentration ($\kappa$), and precision ($\alpha$) for active Energy physics rather than basic coordinate points.
- **PyTorch Integration:** Seamless Native PyTorch `IterableDataset` exported instantly via `m2m.export_to_dataloader()`.

---

## ðŸ— Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PyTorch Training Loop                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Llama / MLP   â”‚ â”‚    Optimizer    â”‚ â”‚   Criterion  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ (Streaming Tensors)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               M2M Iterable Dataset (Data Lake)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Tier-Aware     â”‚ â”‚ Langevin Generatâ”‚ â”‚   SOC Import â”‚  â”‚
â”‚  â”‚  Prefetching    â”‚ â”‚ ive Augmentationâ”‚ â”‚   Sampling   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ (Tiered Storage)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   M2M Memory Engine                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Hot (VRAM)    â”‚ â”‚   Warm (RAM)    â”‚ â”‚  Cold (SSD)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Benchmarks

*Validated with 10,000 real-world high-dimensional structured embeddings (digits projected to S^639):*

### Data Lake Training Throughput (splats/sec)
| Hardware & Mode | Standard Training (SOC) | Generative Training (Langevin) |
| :--- | :--- | :--- |
| **CPU Math** | ~49,368 splats/sec | ~34,993 splats/sec |
| **Vulkan GPU** | ~35,801 splats/sec | **~38,059 splats/sec** |

*Data Ingest Speed into memory tiers:* **~80,260 splats/sec**
*Note: Standard iteration is heavily memory bound, making CPU faster for pure iteration. Generative Langevin dynamics require high numerical compute, where Vulkan acceleration shines, bypassing CPU bottlenecks.*

### Semantic MoE Retrieval Performance (10,000 Splats)
| Hardware | QPS | p95 Latency | p99 Latency | Avg Latency |
| :--- | :--- | :--- | :--- | :--- |
| **CPU Math** | ~62.5 QPS | 19.55 ms | 22.55 ms | 16.00 ms |
| **Vulkan GLSL Shaders** | ~47.0 QPS | 25.01 ms | 29.97 ms | 21.28 ms |

---

## ðŸš€ Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- NumPy 1.21+

### From Source

```bash
git clone https://github.com/schwabauerbriantomas-gif/m2m-vector-search.git
cd m2m-vector-search
git checkout feature/data-lake
pip install -r requirements.txt
```

---

## ðŸš€ Quick Start

### M2M Retrieval and Data Lake in 15 lines of code

```python
import torch
from m2m import M2MConfig, create_m2m

# 1. Initialize M2M Engine (Vulkan Accelerated)
m2m = create_m2m(M2MConfig(device='cpu', enable_vulkan=True))

# 2. Ingest your dataset (embeddings) and Build Hierarchical Index
dataset_embeddings = torch.randn(100000, 640)
m2m.add_splats(dataset_embeddings)
m2m.splats.build_index()

# 3. Progressive Fast Retrieval (RAG)
query = torch.randn(640)
# LOD 0: Ultra-fast Approximation (< 1ms)
approx_results = m2m.retrieve(query, k=10, lod=0) 
# LOD 2: Exact Vulkan SPIR-V Routing (~20ms)
exact_results = m2m.retrieve(query, k=10, lod=2)

# 4. Optional: Export to an Iterable PyTorch DataLoader for Training
dataloader = m2m.export_to_dataloader(
    batch_size=256, 
    generate_samples=True,  # Enables Langevin generative augmentation
    importance_sampling=True # Sorts stream by SOC concentration
)

for epoch in range(1):
    for batch in dataloader:
        # Train your neural network directly on active M2M Splats
        pass
```

See the `/examples` directory for complete working examples, such as `examples/m2m_training_loop.py` and `examples/validate_data_lake.py`.

---

## ðŸ¤ Contributing
We welcome contributions! Please see the issue tracker for feature proposals and bug reports. 

## ðŸ“„ License
Licensed under the Apache License, Version 2.0 (the "License"). You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.
